"""
üé® Self-Hosted Kilim Motif Generator v2 ‚Äî SDXL Turbo (CPU)
Daha kaliteli image generation ‚Äî SDXL tabanlƒ±, 2-5dk/image
"""
import os
import io
import base64
import time
import logging
from contextlib import asynccontextmanager

from fastapi import FastAPI
from pydantic import BaseModel
from typing import Optional
from PIL import Image, ImageFilter, ImageOps

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("image-gen")

MODEL_DIR = os.environ.get("MODEL_DIR", "/models")
MODEL_ID = os.environ.get("MODEL_ID", "stabilityai/sdxl-turbo")
NUM_THREADS = int(os.environ.get("NUM_THREADS", "0"))
PORT = int(os.environ.get("PORT", "8080"))

# Global pipelines
pipe_t2i = None
pipe_i2i = None
model_ready = False

KILIM_PROMPT = """masterpiece, best quality, professional traditional Anatolian Turkish kilim carpet motif, 
highly detailed geometric folk art pattern, pixel-perfect stepped lines, diamond shapes, 
triangle borders, zigzag edges, elibelinde motif style, 
rich deep crimson red, royal navy blue, antique gold saffron, natural cream ivory wool, dark walnut brown, 
flat woven textile texture, zero gradients, zero shadows, authentic hand-woven kilim aesthetic, 
symmetric composition, ornate decorative kilim border frame, museum quality Turkish rug design,
viewed from directly above, flat lay photography of a real kilim rug"""

NEGATIVE_PROMPT = """blurry, low quality, deformed, ugly, disfigured, photorealistic person, face, 
3d render, gradient lighting, shadow, modern art, abstract expressionism, 
watercolor, oil painting, pencil sketch, cartoon, anime,
text, watermark, signature, logo, frame"""


def load_model():
    """SDXL Turbo model y√ºkle"""
    global pipe_t2i, pipe_i2i, model_ready
    import torch
    from diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image

    logger.info(f"üì¶ Model y√ºkleniyor: {MODEL_ID}")
    logger.info(f"üìÇ Cache dizini: {MODEL_DIR}")
    start = time.time()

    if NUM_THREADS > 0:
        torch.set_num_threads(NUM_THREADS)
        logger.info(f"üßµ Thread sayƒ±sƒ±: {NUM_THREADS}")
    else:
        logger.info(f"üßµ Thread sayƒ±sƒ±: otomatik ({torch.get_num_threads()})")

    # SDXL Turbo ‚Äî text-to-image
    pipe_t2i = AutoPipelineForText2Image.from_pretrained(
        MODEL_ID,
        torch_dtype=torch.float32,
        cache_dir=MODEL_DIR,
        variant="fp16" if torch.cuda.is_available() else None,
        safety_checker=None,
    )

    # SDXL Turbo ‚Äî img2img (model aƒüƒ±rlƒ±klarƒ±nƒ± payla≈üƒ±r)
    pipe_i2i = AutoPipelineForImage2Image.from_pipe(pipe_t2i)

    elapsed = time.time() - start
    model_ready = True
    logger.info(f"‚úÖ SDXL Turbo model hazƒ±r! ({elapsed:.1f}s)")


@asynccontextmanager
async def lifespan(app: FastAPI):
    load_model()
    yield


app = FastAPI(title="Kilim Motif Generator v2 ‚Äî SDXL Turbo", lifespan=lifespan)


class GenerateRequest(BaseModel):
    prompt: Optional[str] = None
    image: Optional[str] = None
    strength: float = 0.65            # orijinal ≈üekli koru, stili deƒüi≈ütir
    steps: int = 4            # SDXL Turbo: 1-4 step optimal
    guidance_scale: float = 0.0   # SDXL Turbo: 0.0 = en iyi
    width: int = 512
    height: int = 512


def preprocess_drawing(img: Image.Image, size: int = 512) -> Image.Image:
    """
    Minimal preprocessing ‚Äî orijinal √ßizimi koru, sadece resize et.
    SDXL Turbo strength=0.65 ile orijinal ≈üekli koruyup kilim stili uygulayacak.
    """
    return img.resize((size, size), Image.LANCZOS)


@app.post("/generate")
def generate(req: GenerateRequest):
    """Image generation ‚Äî text2img veya img2img"""
    if not model_ready:
        return {"error": "Model hen√ºz y√ºklenmiyor"}, 503

    prompt = req.prompt or KILIM_PROMPT
    start = time.time()

    try:
        if req.image:
            logger.info(f"üñºÔ∏è img2img ba≈ülƒ±yor (strength={req.strength}, steps={req.steps})")
            img_data = req.image.split(",")[1] if "," in req.image else req.image
            input_img = Image.open(io.BytesIO(base64.b64decode(img_data))).convert("RGB")

            # Preprocessing
            processed_img = preprocess_drawing(input_img, req.width)
            logger.info("üé® Preprocessing tamamlandƒ±")

            result = pipe_i2i(
                prompt=prompt,
                image=processed_img,
                num_inference_steps=req.steps,
                guidance_scale=req.guidance_scale,
                strength=req.strength,
            )
        else:
            logger.info(f"‚úèÔ∏è text2img ba≈ülƒ±yor (steps={req.steps})")
            result = pipe_t2i(
                prompt=prompt,
                num_inference_steps=req.steps,
                guidance_scale=req.guidance_scale,
                width=req.width,
                height=req.height,
            )

        output_img = result.images[0]
        buf = io.BytesIO()
        output_img.save(buf, format="PNG")
        b64 = base64.b64encode(buf.getvalue()).decode()

        elapsed = time.time() - start
        logger.info(f"‚úÖ Image √ºretildi ({elapsed:.1f}s)")

        return {
            "image": f"data:image/png;base64,{b64}",
            "elapsed_seconds": round(elapsed, 1),
        }

    except Exception as e:
        logger.error(f"‚ùå Generation hatasƒ±: {e}")
        return {"error": str(e)}, 500


@app.get("/health")
def health():
    import torch
    return {
        "status": "ready" if model_ready else "loading",
        "model": MODEL_ID,
        "threads": torch.get_num_threads() if model_ready else None,
    }


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=PORT)
